---
date: '2024-12-04T23:12:44+08:00'
draft: true
title: 'About Me'
author: 'hsd'
author_profile: true
imageUrl: "images/mainfig.png"
social_links:
  - name: "twitter"
    url: https://x.com/DeanHu11
    icon: "twitter"
---


I am a final-year PhD student from [the Department of Computer Science and Technology](http://www.cs.tsinghua.edu.cn/), Tsinghua University. I am fortunately supervised by Professor [Maosong Sun](https://www.cs.tsinghua.edu.cn/csen/info/1180/4033.htm) and  [Zhiyuan Liu](http://nlp.csai.tsinghua.edu.cn/~lzy/). In the summer of 2019, I visited [MILA](https://mila.quebec/en/) and conducted research under [Professor Jian Tang](https://jian-tang.com).  

My current passion revolves around building **SCALABLE** solutions to AGI, which means the solutions will bring improvement simply with more resources on computation and data. This include:

1. **Scaling Pretrain**. Ensuring the growth of language models' ability is measureable and predictable.
  
2. **Scaling RL**. This includes developing scaling principles for scalable oversight, PPO, MCTS, etc. 
  
3. **Scaling World Model**  Unifing modality and training objectives, then scaling with pretrain and RL. 
   

In fact, I believe that we will not have achieved AGI until the model is capable of conducting scientific research independently. All the aforementioned points contribute to this objective.


<!-- The nature of intelligence always interests me. Currently I think it might take the form:

 $$I := - \int_{\mathcal{P}} E_0 \frac{\mathrm{d}E}{E}$$

where $I$ is the amount of intelligence, $E$ is information entropy, $E_0$ is the information entropy before applying intelligence, and $\mathcal{P}$ is the joint probability of the information over the world. Note that this definition is only used to express a belief that is neither rigorous nor verified. -->
<!-- more thoughts are needed here. -->


# Selected Publications


* [LEGENT: Open Platform for Embodied Agents](https://arxiv.org/abs/2404.18243). *Arxiv*  
  A Unity 3D platform for building your embodied agents powered by LLMs. Still under development! If you want to be a contributor please contact me via email.


* [Unified View of Grokking, Double Descent and Emergent Abilities: A Perspective from Circuits Competition](https://arxiv.org/abs/2402.15175). *Arxiv*  
  
  <i>A simplest and beautiful <span style="color: red;">mechanistic interpretability</span> work. Providing perspective to understand the fascinating phenomena during model scaling and data scaling, encompassing Grokking, Double Descent, and Emergent Abilities.</i>
  
  Yufei Huang, **Shengding Hu**, Xu Han, Zhiyuan Liu, Maosong Sun

* [MiniCPM: Unveiling the Potential of End-side Large Language Models](https://arxiv.org/abs/2404.06395). *Arxiv*  
  
  <i>A small LLM with 2.4B non-embedding parameters that rivals Llama-13B or Mistral-7B. With several scaling experiments. <span style="color: red;">WSD learning rate scheduler</span> is proposed as a substitude for Cosine learning scheduler. <span style="color: red;">Trending on Github and Huggingface.</span> </i>

* [OlympiadBench: A Challenging Benchmark for Promoting AGI with Olympiad-Level Bilingual Multimodal Scientific Problems](https://arxiv.org/abs/2402.14008). *Arxiv*  
  
  <i>A benchmark that challenges AGI with humanity's most eminent intellectual contests, serving as a beacon for future AGI development and a platform for studying scalable oversight. </i>

  Chaoqun He, Renjie Luo, Yuzhuo Bai, **Shengding Hu**, Zhen Leng Thai, Junhao Shen, Jinyi Hu, Xu Han, Yujie Huang, Yuxiang Zhang, Jie Liu, Lei Qi, Zhiyuan Liu, Maosong Sun

* [âˆžBench: Extending Long Context Evaluation Beyond 100K Tokens](https://arxiv.org/abs/2402.14008). *Arxiv*  
  
  <i>Benchmark for super long context. Long context is almost everything (better in an efficient way)</i>

  Xinrong Zhang, Yingfa Chen, **Shengding Hu**, Zihang Xu, Junhao Chen, Moo Khai Hao, Xu Han, Zhen Leng Thai, Shuo Wang, Zhiyuan Liu, Maosong Sun


* [Predicting Emergent Abilities with Infinite Resolution Evaluation](https://arxiv.org/abs/2310.03262). *Preprint*  
  
  <span style="color: red;">The first work that achieves predictable scaling besides GPT-4</span>
  
  **Shengding Hu**, Xin Liu, Xu Han, Xinrong Zhang, Chaoqun He, Weilin Zhao, Yankai Lin, Ning Ding, Zebin Ou, Guoyang Zeng, Zhiyuan Liu, Maosong Sun
  
* [Won't Get Fooled Again: Answering Questions with False Premises](https://arxiv.org/pdf/2307.02394)  
  
  <span style="color: red;">ACL Oral Representation</span>
  
  **Shengding Hu**, Yifan Luo, Huadong Wang, Xingyi Cheng, Zhiyuan Liu, Maosong Sun


* [Tool Learning with Foundation Models](https://arxiv.org/abs/2304.08354) *Preprints*.

  <span style="color: red;">A 75 page study of LLM tool use. </span>

  Yujia Qin, **Shengding Hu**, Yankai Lin, Weize Chen, Ning Ding, Ganqu Cui, ...(other 33 co-authors), Zhiyuan Liu, Maosong Sun.


* [OpenPrompt: An Open-source Framework for Prompt-learning](https://arxiv.org/abs/2111.01998) *ACL 2022 Demo*  
  
  <span style="color: red;">ACL 2022 Best Demo Award</span>
  
  Ning Ding\*, **Shengding Hu**\*, Weilin Zhao\*, Yulin Chen, Zhiyuan Liu, Hai-Tao Zheng, Maosong Sun


* [Knowledgeable Prompt-tuning: Incorporating Knowledge into Prompt Verbalizer for Text Classification.](https://arxiv.org/abs/2108.02035) *ACL 2022.* 
  
  <span style="color: red;">More than 200 citations</span>
  
  **Shengding Hu**, Ning Ding, Huadong Wang, Zhiyuan Liu, JinGang Wang, Juanzi Li, Wei Wu,  Maosong Sun 


* [Delta tuning: A comprehensive study of parameter efficient methods for pre-trained language models](https://arxiv.org/abs/2203.06904) *Preprint* 
  
  <span style="color: red;">Neural Machine Intelligence Cover Article</span>
  
  Ning Ding\*, Yujia Qin\*, Guang Yang, Fuchao Wei, Zonghan Yang, Yusheng Su, **Shengding Hu**, Yulin Chen, Chi-Min Chan, Weize Chen, Jing Yi, Weilin Zhao, Xiaozhi Wang, Zhiyuan Liu, Hai-Tao Zheng, Jianfei Chen, Yang Liu, Jie Tang, Juanzi Li, Maosong Sun

* [Graph Neural Networks: A Review of Methods and Applications.](https://arxiv.org/abs/1812.08434) *AI Open 2021.* 
  
  <span style="color: red;">More than 4000 citations</span>
  
  Jie Zhou\*, Ganqu Cui\*, **Shengding Hu**, Zhengyan Zhang, Cheng Yang, Zhiyuan Liu, Lifeng Wang, Changcheng Li, Maosong Sun. 

For other papers, please refer to my [google scholar](https://scholar.google.com/citations?user=ZfehPhAAAAAJ&hl=en&oi=ao)

# Selected Projects

[![Readme Card](https://github-readme-stats.vercel.app/api/pin/?username=OpenBMB&repo=MiniCPM)](https://github.com/OpenBMB/MiniCPM) 

[![Readme Card](https://github-readme-stats.vercel.app/api/pin/?username=thunlp&repo=OpenDelta)](https://github.com/thunlp/OpenDelta) 

[![Readme Card](https://github-readme-stats.vercel.app/api/pin/?username=thunlp&repo=OpenPrompt)](https://github.com/thunlp/OpenPrompt)

[![Readme Card](https://github-readme-stats.vercel.app/api/pin/?username=thunlp&repo=PromptPapers)](https://github.com/thunlp/PromptPapers)

[![Readme Card](https://github-readme-stats.vercel.app/api/pin/?username=OpenBMB&repo=InfiniteBench)](https://github.com/OpenBMB/InfiniteBench)

[![Readme Card](https://github-readme-stats.vercel.app/api/pin/?username=OpenBMB&repo=BMPrinciples)](https://github.com/OpenBMB/BMPrinciples)


# Awards

### Master&PhD

  
- National Natural Science Foundation of China (NSFC) Doctoral Project Leader (~1/100 among all subjects and all institutes in China)

- [Siebel Scholar of Class 2023](https://www.businesswire.com/news/home/20220922005006/en/Siebel-Scholars-Foundation-Announces-Class-of-2023) 
  
  *1/83 across the world, Price $30,000* 

- National Scholarship 2021. 

  One of the highest award in Tsinghua University.

### Bachelor
- Academic Excellence Award in 2016-2017 , 2017-2018, 2018-2019.
- Good reading scholarship in 2018-2019 year.
- Zhuzhou Scholarship in 2017-2018 year

### Earlier
- Silver Medal in the 32nd National Middle School Physics Competition
